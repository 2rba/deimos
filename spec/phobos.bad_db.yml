logger:
  # Optional log file, set to false or remove to disable it
  file: log/phobos.log
  # Optional output format for stdout, default is false (human readable).
  # Set to true to enable json output.
  stdout_json: false
  level: debug
  # Comment the block to disable ruby-kafka logs
  ruby_kafka:
    level: debug

kafka:
  # identifier for this application
  client_id: phobos
  # timeout setting for connecting to brokers
  connect_timeout: 15
  # timeout setting for socket connections
  socket_timeout: 15

producer:
  # number of seconds a broker can wait for replicas to acknowledge
  # a write before responding with a timeout
  ack_timeout: 5
  # number of replicas that must acknowledge a write, or `:all`
  # if all in-sync replicas must acknowledge
  required_acks: 1
  # number of retries that should be attempted before giving up sending
  # messages to the cluster. Does not include the original attempt
  max_retries: 2
  # number of seconds to wait between retries
  retry_backoff: 1
  # number of messages allowed in the buffer before new writes will
  # raise {BufferOverflow} exceptions
  max_buffer_size: 10000
  # maximum size of the buffer in bytes. Attempting to produce messages
  # when the buffer reaches this size will result in {BufferOverflow} being raised
  max_buffer_bytesize: 10000000
  # name of the compression codec to use, or nil if no compression should be performed.
  # Valid codecs: `:snappy` and `:gzip`
  compression_codec:
  # number of messages that needs to be in a message set before it should be compressed.
  # Note that message sets are per-partition rather than per-topic or per-producer
  compression_threshold: 1
  # maximum number of messages allowed in the queue. Only used for async_producer
  max_queue_size: 10000
  # if greater than zero, the number of buffered messages that will automatically
  # trigger a delivery. Only used for async_producer
  delivery_threshold: 0
  # if greater than zero, the number of seconds between automatic message
  # deliveries. Only used for async_producer
  delivery_interval: 0

consumer:
  # number of seconds after which, if a client hasn't contacted the Kafka cluster,
  # it will be kicked out of the group
  session_timeout: 300
  # interval between offset commits, in seconds
  offset_commit_interval: 10
  # number of messages that can be processed before their offsets are committed.
  # If zero, offset commits are not triggered by message processing
  offset_commit_threshold: 0
  # interval between heartbeats; must be less than the session window
  heartbeat_interval: 10

backoff:
  min_ms: 1000
  max_ms: 60000

listeners:
  - handler: ConsumerTest::MyConsumer
    topic: my_consume_topic
    group_id: my_group_id
    max_bytes_per_partition: 524288 # 512 KB
